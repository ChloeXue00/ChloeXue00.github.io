<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>TD3 on </title>
    <link>//localhost:1313/tags/td3/</link>
    <description>Recent content in TD3 on </description>
    <generator>Hugo -- 0.147.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 17 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/td3/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何训练用于狭窄通道规划的 DRL 策略？</title>
      <link>//localhost:1313/posts/narrow_exploration/</link>
      <pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/posts/narrow_exploration/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;狭窄通道&#34;&gt;在这个混合路径规划系统中，DRL 策略负责接管那些局部空间极端受限、传统 MPC 难以生效的区域，特别是“狭窄通道（narrow passage）”问题。为此，我们构建了一个专门应对该类场景的 DRL 策略，并通过 TD3 或 DDPG 算法进行训练。
&lt;img alt=&#34;狭窄通道&#34; loading=&#34;lazy&#34; src=&#34;//localhost:1313/images/hybrid_image_result.png&#34;&gt;&lt;/h2&gt;
&lt;h2 id=&#34;1-使用的算法与损失函数&#34;&gt;1. 使用的算法与损失函数&lt;/h2&gt;
&lt;p&gt;我们使用 &lt;strong&gt;TD3 或 DDPG&lt;/strong&gt; 算法，分别训练策略网络（Actor）和价值网络（Critic）。&lt;/p&gt;
&lt;h3 id=&#34;ddpg&#34;&gt;DDPG&lt;/h3&gt;
&lt;p&gt;DDPG 有时能够实现出色的性能，但它在超参数和其他类型的调优方面往往很脆弱。DDPG 的一个常见故障模式是，学习到的 Q 函数开始大幅高估 Q 值，从而导致策略破坏，因为它利用了 Q 函数中的误差。&lt;/p&gt;
&lt;h3 id=&#34;td3&#34;&gt;TD3&lt;/h3&gt;
&lt;p&gt;td3 在DDPG基础上做到了三个技巧的更新，解决DDPG Q值过高的问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;技巧1： 裁剪双Q学习
TD3学习两个Q函数 而不是一个（因此称为twin），并使用两个Q值比较小的一个作为bellman误差损失函数中的目标&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技巧2： “延迟”策略更新
TD3 更新策略和目标网络的频率低于Q函数，本文建议没更新两次Q函数就进行依次策略更新&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技巧2： 目标策略平滑
TD3为目标动作添加了噪声，通过平滑动作中的Q的变化，使策略更难利用Q函数误差
这三个技巧可以显著提高baseline DDPG 的性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD3 是一种off-policy algorithm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD3 只能用于具有连续动作空间的环境&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TD3 的Spinning up实现不支持并行化&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关键方程式：
TD3通过 均方bellman误差最小化的同时 学习两个Q函数Q_phi_1和Q_phi_2， 其方式于DDPG学习单个Q函数的方式几乎相同，
为了准确展示TD3的实现方式， 以及它与普通DDPG的区别，我们将从损失函数的最内层向外进行讲解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一：目标策略平滑
用于构成Q学习目标的动作 基于目标策略/mu_theta_targ , 但在动作的每个维度上添加了截断噪声。添加阶段噪声后，目标动作将被阶段， 使其位于有效动作范围内（所有有效动作，都满足alpha_low &amp;lt;= alpha &amp;lt;= alpha_high）。因此，目标动作如下：
&lt;img alt=&#34;目标动作方程&#34; loading=&#34;lazy&#34; src=&#34;//localhost:1313/images/td3_target_action.png&#34;&gt;
目标策略平滑本质上充当了算法的正则化器（正则化是一组用于减少机器学习模型中过拟合的方法。正则化会用训练准确性的边际下降来换取泛化性的提高。 正则化包含一系列用于纠正机器学习模型过拟合问题的方法。）
它解决了DDPG中可能出现的一种特殊故障模式：如果Q函数逼近器针对某些动作产生了错误的尖峰，策略就会迅速利用改封至，从而导致脆弱或错误的行为。
这种情况可以通过平滑类似动作的Q函数来避免，
而这正是目标策略平滑的设计初衷。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
