<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DRL on </title>
    <link>/categories/drl/</link>
    <description>Recent content in DRL on </description>
    <generator>Hugo -- 0.147.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 17 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/drl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DRL 常见算法对比</title>
      <link>/posts/drl-algorithm-comparison/</link>
      <pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate>
      <guid>/posts/drl-algorithm-comparison/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;深度强化学习（Deep RL）发展出多个主流算法流派，包括基于值函数的 DQN、基于策略梯度的 REINFORCE/PPO，以及融合策略和值函数的 Actor-Critic 框架（如 A2C、DDPG、SAC）。每种方法适用于不同场景，选择合适算法将显著影响模型性能与训练效率。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;三大算法流派对比&#34;&gt;三大算法流派对比&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;算法类型&lt;/th&gt;
          &lt;th&gt;特点&lt;/th&gt;
          &lt;th&gt;优点&lt;/th&gt;
          &lt;th&gt;局限&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Value-based&lt;/td&gt;
          &lt;td&gt;学习 Q(s, a) 并通过贪婪策略选动作&lt;/td&gt;
          &lt;td&gt;样本利用率高，适合离散动作空间&lt;/td&gt;
          &lt;td&gt;不适用于连续/高维动作&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Policy-based&lt;/td&gt;
          &lt;td&gt;直接建模并优化策略 π(a&lt;/td&gt;
          &lt;td&gt;s)&lt;/td&gt;
          &lt;td&gt;适合连续动作，训练稳定&lt;/td&gt;
          &lt;td&gt;样本效率低，梯度方差大&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Actor-Critic&lt;/td&gt;
          &lt;td&gt;同时训练策略和价值函数&lt;/td&gt;
          &lt;td&gt;综合两者优势，适用于复杂控制问题&lt;/td&gt;
          &lt;td&gt;架构复杂，对超参数敏感&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一dqndeep-q-network&#34;&gt;一、DQN（Deep Q-Network）&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;类型&lt;/strong&gt;：Value-based&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;损失函数（均方 TD 误差）&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L(θ) = 𝔼ₜ [(rₜ + γ · maxₐ′ Qθ⁻(sₜ₊₁, a′) − Qθ(sₜ, aₜ))²]&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;θ 是 Q 网络参数&lt;/li&gt;
&lt;li&gt;θ⁻ 是目标网络参数（定期同步）&lt;/li&gt;
&lt;li&gt;使用贪婪策略选择 &lt;code&gt;a′ = argmax Q(s′, a′)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;优化器&lt;/strong&gt;：Adam 或 SGD，通过反向传播最小化 TD 误差更新 θ。&lt;/p&gt;</description>
    </item>
    <item>
      <title>DRL训练</title>
      <link>/posts/how-to-train-drl-model/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      <guid>/posts/how-to-train-drl-model/</guid>
      <description>&lt;p&gt;在深度强化学习（Deep Reinforcement Learning）中，我们的目标是：训练一个智能体（Agent），使其能够在复杂环境中通过交互、试错和学习，掌握解决任务的策略。&lt;/p&gt;
&lt;p&gt;整个训练流程不是一行 &lt;code&gt;.fit()&lt;/code&gt; 就能完成的，它涉及数据采集、策略评估、价值估计、梯度优化等多个协同模块。本文将逐步介绍训练所需的关键模块和完整流程。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;drl-训练流程的核心结构&#34;&gt;DRL 训练流程的核心结构&lt;/h2&gt;
&lt;p&gt;一个标准的 DRL 系统至少包括以下几个部分：&lt;/p&gt;
&lt;h3 id=&#34;1-环境environment&#34;&gt;1. 环境（Environment）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;提供 &lt;code&gt;reset()&lt;/code&gt; 和 &lt;code&gt;step(action)&lt;/code&gt; 接口&lt;/li&gt;
&lt;li&gt;返回状态、奖励、终止信号和调试信息&lt;/li&gt;
&lt;li&gt;通常使用 Gymnasium 编写，也可以是仿真器（如 PyBullet、AirSim）或实际系统接口&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-策略网络policy-network&#34;&gt;2. 策略网络（Policy Network）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;输入当前状态，输出一个动作（或动作分布）&lt;/li&gt;
&lt;li&gt;对于离散动作空间，常见输出为 softmax 分布；连续动作空间则直接输出浮点数&lt;/li&gt;
&lt;li&gt;通常是 MLP（结构化输入）或 CNN（图像输入）网络&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-值函数网络critic-可选&#34;&gt;3. 值函数网络（Critic, 可选）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;用于评估当前策略下某状态的“好坏”，即状态值 V(s) 或动作值 Q(s,a)&lt;/li&gt;
&lt;li&gt;在 Actor-Critic 架构中，Actor 提出动作，Critic 提供反馈&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-回放缓存replay-buffer&#34;&gt;4. 回放缓存（Replay Buffer）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;保存经验 &lt;code&gt;(state, action, reward, next_state, done)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;支持随机采样，避免训练中数据高度相关&lt;/li&gt;
&lt;li&gt;对于 off-policy 算法（如 DDPG、TD3）是必要组件&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5-优化器与更新规则optimizer&#34;&gt;5. 优化器与更新规则（Optimizer）&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;根据策略梯度、TD 误差、KL 散度等损失函数对网络参数进行更新&lt;/li&gt;
&lt;li&gt;通常使用 Adam 优化器&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;强化学习算法的三种主流架构&#34;&gt;强化学习算法的三种主流架构&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方法类别&lt;/th&gt;
          &lt;th&gt;特点&lt;/th&gt;
          &lt;th&gt;代表算法&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Policy-based&lt;/td&gt;
          &lt;td&gt;直接建模并优化策略 π(a,s)，通过最大化期望回报更新策略&lt;/td&gt;
          &lt;td&gt;REINFORCE, PPO, TRPO&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Value-based&lt;/td&gt;
          &lt;td&gt;学习动作价值函数 Q(s,a)，通过贪婪策略导出动作选择&lt;/td&gt;
          &lt;td&gt;DQN, Double DQN, Dueling DQN&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Actor-Critic&lt;/td&gt;
          &lt;td&gt;同时学习策略和价值函数，策略用于决策，价值函数用于评估&lt;/td&gt;
          &lt;td&gt;A2C, A3C, DDPG, TD3, SAC, PPO&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Policy-based&lt;/strong&gt; 方法训练稳定性强、适合高维动作空间，但样本效率较低。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Value-based&lt;/strong&gt; 方法样本效率较高，适用于离散动作任务，但连续控制较困难。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor-Critic&lt;/strong&gt; 综合两者优势，是当前主流算法的主干框架。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;一个完整训练循环的结构&#34;&gt;一个完整训练循环的结构&lt;/h2&gt;
&lt;p&gt;以下是一个 off-policy 强化学习算法（如 DDPG）的大致流程：&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习环境之 Gymnasium</title>
      <link>/posts/what-is-gymnasium/</link>
      <pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate>
      <guid>/posts/what-is-gymnasium/</guid>
      <description>&lt;p&gt;&lt;code&gt;Gymnasium&lt;/code&gt; 是 Python 中一个标准化的强化学习环境库（它是原始 &lt;code&gt;OpenAI Gym&lt;/code&gt; 的升级版）。它的作用是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;把复杂问题（如机器人走路、自驾车避障）包装成统一的“游戏接口”，让强化学习模型可以跟它反复互动、学会做决策。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;换句话说，它是一个统一标准，让算法和环境能“说上话”。只要环境符合 Gymnasium 接口，你就能直接套用主流算法如 PPO、DDPG、DQN 去训练。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;如何安装-gymnasium&#34;&gt;如何安装 Gymnasium？&lt;/h2&gt;
&lt;p&gt;在终端输入以下命令安装（建议使用虚拟环境）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install gymnasium&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;all&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果你只用基本环境，不包括 Atari、Box2D 等，可以简化为：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install gymnasium
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;在代码中如何导入并使用&#34;&gt;在代码中如何导入并使用&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; gymnasium &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; gym
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;env &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gym&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;make(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;CartPole-v1&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;obs, info &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    action &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;action_space&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sample()  &lt;span style=&#34;color:#75715e&#34;&gt;# 随机动作（一般训练中由模型决定）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    obs, reward, terminated, truncated, info &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step(action)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; terminated &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; truncated:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        obs, info &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reset()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这段代码展示了标准的强化学习交互流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始化环境（&lt;code&gt;reset()&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;持续循环：选择动作 → 执行动作 → 接收反馈&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;环境的输入与输出env-接口结构&#34;&gt;环境的输入与输出：Env 接口结构&lt;/h2&gt;
&lt;h3 id=&#34;输入action&#34;&gt;输入：action&lt;/h3&gt;
&lt;p&gt;你给环境的输入是一个“动作”，比如：&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习的状态空间和动作空间是什么</title>
      <link>/posts/state-action-space/</link>
      <pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate>
      <guid>/posts/state-action-space/</guid>
      <description>&lt;p&gt;在强化学习中，状态空间（Observation Space）和动作空间（Action Space）是两个基础概念。理解这两个空间，就等于弄清楚“模型看到了什么”和“模型可以做什么”。&lt;/p&gt;
&lt;p&gt;在本项目中，我们使用的是基于图像和传感器输入的强化学习模型，用于端到端的控制任务，如预测方向、速度或操作行为。下面我们从通用概念讲起，最后具体说明本项目的定义。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;状态空间observation-space&#34;&gt;状态空间（Observation Space）&lt;/h2&gt;
&lt;p&gt;状态空间描述了智能体每一步能从环境中获取到的“状态信息”。这些信息构成了模型的输入，可以是一个向量，也可以是图像、组合信号等。&lt;/p&gt;
&lt;h3 id=&#34;常见类型&#34;&gt;常见类型：&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类型&lt;/th&gt;
          &lt;th&gt;示例&lt;/th&gt;
          &lt;th&gt;含义说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;Box(4,)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;[-4.8, 4.8]&lt;/code&gt; × 4&lt;/td&gt;
          &lt;td&gt;连续变量向量，如位置、速度等&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;Box(84, 84, 3)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;图像输入&lt;/td&gt;
          &lt;td&gt;视觉输入，常用于端到端控制&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;Dict(...)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;多通道输入&lt;/td&gt;
          &lt;td&gt;图像 + 雷达 + IMU 组合观测&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;本项目中的状态空间定义&#34;&gt;本项目中的状态空间定义：&lt;/h3&gt;
&lt;p&gt;在本项目中，我们的状态空间通常包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;图像帧&lt;/strong&gt;：从机器人前置摄像头获取的 RGB 图像（如 &lt;code&gt;(224, 224, 3)&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;附加状态向量&lt;/strong&gt;（可选）：如上一帧速度、转角、航向角等传感器读数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;状态空间结构可能是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Box(low&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, high&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;, shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;uint8)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Dict({
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;: Box(&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;speed&amp;#34;&lt;/span&gt;: Box(&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;yaw&amp;#34;&lt;/span&gt;: Box(&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;})
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;动作空间action-space&#34;&gt;动作空间（Action Space）&lt;/h2&gt;
&lt;p&gt;动作空间描述了模型每一步可以采取的动作范围，也就是输出的结构。&lt;/p&gt;
&lt;h3 id=&#34;常见类型-1&#34;&gt;常见类型：&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;类型&lt;/th&gt;
          &lt;th&gt;示例&lt;/th&gt;
          &lt;th&gt;用途说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;Discrete(n)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;Discrete(3)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;离散控制，如左、右、直行&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;Box(...)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;Box(-1, 1, (2,), float32)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;连续动作，如转角和加速度控制&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;MultiBinary(n)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;MultiBinary(5)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;多位二进制开关组合&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;本项目中的动作空间定义&#34;&gt;本项目中的动作空间定义：&lt;/h3&gt;
&lt;p&gt;我们通常使用连续动作空间来控制机器人，例如：&lt;/p&gt;</description>
    </item>
    <item>
      <title>数据结构与算法(自动驾驶应用方向)</title>
      <link>/posts/data_structure_and_algorithm/</link>
      <pubDate>Thu, 02 May 2024 00:00:00 +0000</pubDate>
      <guid>/posts/data_structure_and_algorithm/</guid>
      <description>&lt;h1 id=&#34;数据结构与算法自动驾驶应用方向&#34;&gt;数据结构与算法（自动驾驶应用方向）&lt;/h1&gt;
&lt;p&gt;在自动驾驶中，无论是路径规划、障碍物识别，还是实时控制，都离不开对&lt;strong&gt;数据的高效组织和计算方法的选择&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-常见数据结构及自动驾驶应用&#34;&gt;2. 常见数据结构（及自动驾驶应用）&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;数据结构&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
          &lt;th&gt;应用示例&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;数组 / 向量&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;连续存储，支持随机访问&lt;/td&gt;
          &lt;td&gt;储存历史轨迹点、车辆状态序列&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;链表 / 环形队列&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;灵活插入/删除&lt;/td&gt;
          &lt;td&gt;控制器中缓存最近速度或传感器数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;栈 / 队列&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;先进后出 / 先进先出&lt;/td&gt;
          &lt;td&gt;控制状态切换、命令排队处理&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;哈希表（Hash Map）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;快速查找&lt;/td&gt;
          &lt;td&gt;建图中障碍物记录，传感器缓存索引&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;堆（Heap）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;优先级队列&lt;/td&gt;
          &lt;td&gt;A*/Dijkstra路径搜索中寻找最短路径&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;树 / 四叉树 / 八叉树&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;分层组织空间&lt;/td&gt;
          &lt;td&gt;空间分割（如QuadTree用于2D路径规划）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;图（Graph）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;节点+边&lt;/td&gt;
          &lt;td&gt;路径规划地图、任务依赖图等&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-算法在自动驾驶中的典型应用&#34;&gt;3. 算法在自动驾驶中的典型应用&lt;/h2&gt;
&lt;h3 id=&#34;31-搜索与路径规划算法&#34;&gt;3.1 搜索与路径规划算法&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;算法&lt;/th&gt;
          &lt;th&gt;应用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A*&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;地图中的最短路径搜索&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Dijkstra&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;全局路径规划&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;RRT/RRT*&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;采样路径规划，适用于复杂动态场景&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;32-数学优化类算法&#34;&gt;3.2 数学优化类算法&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;算法&lt;/th&gt;
          &lt;th&gt;应用&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;MPC（Model Predictive Control）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;控制指令生成，预测未来行为&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;最小均方（LMS）/最小二乘（LS）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;滤波器设计，状态估计等&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;33-搜索结构加速&#34;&gt;3.3 搜索结构加速&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;技术&lt;/th&gt;
          &lt;th&gt;描述&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;KD-Tree&lt;/td&gt;
          &lt;td&gt;快速搜索最近邻（如点云比对）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Hashing&lt;/td&gt;
          &lt;td&gt;快速定位地图块、路段编号&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-4-示例a-路径规划伪代码&#34;&gt;🧪 4. 示例：A* 路径规划（伪代码）&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;A_star&lt;/span&gt;(start, goal, map):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    open_list &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PriorityQueue()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    open_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;put(start)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; open_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;empty():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        current &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; current &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; goal:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; reconstruct_path()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; neighbor &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; get_neighbors(current):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; visited(neighbor):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; calculate_cost(current, neighbor)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                open_list&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;put(neighbor, cost)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;-推荐学习资料&#34;&gt;📘 推荐学习资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;《算法图解》：通俗易懂的入门书&lt;/li&gt;
&lt;li&gt;《数据结构与算法分析》：理论+实现&lt;/li&gt;
&lt;li&gt;LeetCode 专题：图、堆、树、搜索优化&lt;/li&gt;
&lt;li&gt;ROS Navigation Stack 源码分析&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
